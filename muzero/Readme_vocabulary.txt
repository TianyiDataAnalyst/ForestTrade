# -*- coding: utf-8 -*-
"""
Created on Mon May 11 16:49:34 2020

@author: gutia
"""
paper: https://arxiv.org/pdf/1911.08265.pdf
code : https://arxiv.org/src/1911.08265v2/anc/pseudocode.py

Vocabulary 

Reinforcement learning (RL) is an area of machine learning concerned 
with how software agents ought to take actions in an environment in 
order to maximize the notion of cumulative reward. 

Markov decision process (MDP)
It is a discrete time stochastic control process. 
It provides a mathematical framework for modeling decision making in 
situations where outcomes are partly random and partly under the 
control of a decision maker. MDPs are useful for studying optimization 
problems solved via dynamic programming and reinforcement learning. 
    State transition model - predicting the next state
    Reward model - predicting the expected reward during that transition
    MDP planning algorithms - Monte-Carlo tree search (MCTS)
        MCTS - compute the optimal value or optimal policy for the MDP

Three important quantities
policy - the move to play
value function - the predicted winner
immediate reward - the points scored by playing a move


How the algorithm work:
1.construct the state representation that the model should predict.
2. modeling the observation
3. predict those aspects of the future that are directly relevant for 
planning
4. The model receives the observation as an input and transforms 
it into a hidden state
5. The hidden state is then updated iteratively by a recurrent process 
that receives the previous hidden state and a hypothetical next action
6.The model is trained end-to-end, with the sole objective of accurately estimating these
three important quantities, so as to match the improved estimates of policy and value generated by search as well
as the observed reward. 

Predictron
The predictron consists of a fully abstract model, represented by a 
Markov reward process, that can be rolled forward multiple "imagined" 
planning steps. Each forward pass of the predictron accumulates 
internal rewards and values over multiple planning depths. 
The predictron is trained end-to-end so as to make these accumulated 
values accurately approximate the true value function. 
We applied the predictron to procedurally generated random mazes 
and a simulator for the game of pool. The predictron yielded 
significantly more accurate predictions than conventional deep 
neural network architectures.

Compounding error
representation learning - model learning - planning
 the agent is not able to optimize its representation or model 
 for the purpose of effective planning, 
 so that, for example modeling errors may compound during planning.

Despite its potential to improve sample complexity versus model-free 
approaches, model-based reinforcement learning can fail catastrophically
 if the model is inaccurate. An algorithm should ideally be able to 
 trust an imperfect model over a reasonably long planning horizon, 
 and only rely on model-free updates when the model errors get 
 infeasibly large.
 To be investigate more: https://arxiv.org/abs/1912.11206
 
1. Model to plan:
Representation: state s_k, action a_k
Dynamics - g : reward r_k, s_k = g(s_k_m1, a_k)
Prediction - f: policy p_k, v_k = f(s_k)

2. Act in the environment:
MCTS: timestep t, sesearch policy pi_t, action a_ta1

3. Train the model:
Experience replay
 representation function h, observations o1, ..., ot  from the selected trajectory
 for K steps. Each step k, g function receives input as input the hidden 
 state s_km1 and action a_tak
Prediction  policy p_tak, value v_tak, reward r_tak

Loss function
a loss function or cost function is a function that maps an event 
or values of one or more variables onto a real number intuitively 
representing some "cost" associated with the event.